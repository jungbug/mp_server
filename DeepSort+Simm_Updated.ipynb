{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9028d127-cdc3-4f65-b275-3214eb75c8f6",
   "metadata": {},
   "source": [
    "## 숙지사항\n",
    "\n",
    ">1. DeepSort를 사용함 (라이브러리 의존성 없게 코드 개선함)\n",
    "2. YOLOv8 사용하도록 코드 개선 (기존에는 YOLOv3)\n",
    "3. DeepSort의 내부 모델 일부를 뜯어내서 Similarity 모델로 사용\n",
    "4. 가장 비슷한 사람 1명(GreenBox)과 그 외 비슷한 특징을 가진 N명(YellowBox)을 보여줌 (비슷한 특징을 가진 N명은 추 후 선택해서 변경할 수 있도록 할 예정)\n",
    "5. sim_threshold 라는 하이퍼파라미터로 비슷한 특징의 기준 조절가능. 현재로써는 0.09가 적당해보이지만 0.085도 괜찮아보임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c72bfb-be11-4702-bd49-c65fb48a1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_name='test.png'\n",
    "source_video_name='test.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0dcdd8-5049-4b1a-9f72-fab9c50accef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_threshold=0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fafb7-23d3-48be-b472-88cbc1d2a4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66acb55-7cd1-4c83-82ee-f38706151e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ultralytics imageio imageio[ffmpeg] imageio[pyav]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907bad5e-cf14-46ef-a499-2e4591d19923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort import nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "from deep_sort.tracker import Tracker\n",
    "from deep_sort import generate_detections as gdet\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import imageio.v3 as iio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f40e4-cc07-455a-a2ba-37bce0be0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "#image(frame)한장에 대해 객체 검출 후 crop 하는 함수\n",
    "def detect_person(img, conf_threshold=0.5, ret_coord=False):\n",
    "    result=det_model.cpu()(img, verbose=False)\n",
    "    \n",
    "    boxes_out=np.array([np.concatenate([np.array(box.xywh[0])[:2]-np.array(box.xywh[0])[2:]/2,np.array(box.xywh[0])[2:]]) for box in result[0].boxes if box.cls==0 and box.conf>=conf_threshold])\n",
    "    conf_out=np.array([np.array(box.conf) for box in result[0].boxes if box.cls==0 and box.conf>=conf_threshold])\n",
    "    \n",
    "    boxes=np.array([np.array([box.xyxyn[0][1],box.xyxyn[0][0],box.xyxyn[0][3],box.xyxyn[0][2]]) for box in result[0].boxes if box.cls==0 and box.conf>=conf_threshold])\n",
    "    \n",
    "    _img=tf.repeat(img[None], len(boxes), axis=0)\n",
    "\n",
    "    cropped_image=tf.image.crop_and_resize(_img, boxes, range(len(boxes)), (128,64))\n",
    "    \n",
    "    if ret_coord:\n",
    "        return tf.image.crop_and_resize(_img, boxes, range(len(boxes)), (128,64)), boxes_out, conf_out\n",
    "    else:\n",
    "        return tf.image.crop_and_resize(_img, boxes, range(len(boxes)), (128,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928eed4-9db7-4003-b73f-4b661044e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비디오를 frames로 변환\n",
    "frames=iio.imread(Path(source_video_name).read_bytes(), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b065475-5b0d-445a-ba68-9bf09d96fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target image 차원 조정\n",
    "img=iio.imread(target_image_name)[...,:3]\n",
    "#target image에서 객체 crop - 어차피 1명.\n",
    "target=detect_person(img)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb30d23-5588-403c-847c-e2d1630b56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracking 할때 사용할, 가장 거리 작은 id 찾는 함수\n",
    "def similar_id(dictionary, value, threshold=1e-5):\n",
    "    r=None\n",
    "    last=np.inf\n",
    "    for k in dictionary.keys():\n",
    "        if np.abs(value-dictionary[k])<last and np.abs(value-dictionary[k])<threshold:\n",
    "            last=np.abs(value-dictionary[k])\n",
    "            r=k\n",
    "            \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3508928-2559-4cad-9ede-7ba886bfae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cosine_distance = 0.7\n",
    "nn_budget = None\n",
    "\n",
    "model_filename = 'mars-small128.pb'\n",
    "encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "tracker = Tracker(metric)\n",
    "\n",
    "#프레임별로 id와 bbox 좌표 담는 리스트\n",
    "tracked=[]\n",
    "# 트랙 ID 별로 프레임에서 검출된 이미지를 모아두는 딕셔너리.(ID가 key)\n",
    "objects={}\n",
    "\n",
    "for frame in frames:\n",
    "    #frame(단일 이미지)에서 사람 객체 검출 후 저장.\n",
    "    crops,boxes,scores = detect_person(frame,ret_coord=True)\n",
    "    \n",
    "    #객체 이름(사물 이름) person 통일\n",
    "    names = ['person' for _ in range(len(boxes))]\n",
    "    # 한 프레임내의 bbox들에 대해 특징벡터로 변환\n",
    "    features=encoder(frame,boxes)\n",
    "    \n",
    "    # 각 객체의 정보를 담고 있음\n",
    "    detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in zip(boxes, scores, names, features)]\n",
    "    \n",
    "    # 각 객체의 정보를 바탕으로 각 트랙(ID) 상태 업데이트\n",
    "    tracker.predict()\n",
    "    tracker.update(detections)\n",
    "    \n",
    "    \n",
    "    # Obtain info from the tracks\n",
    "    # 각 track별로 현재 프레임의 좌표 저장\n",
    "    tracked_bboxes = []\n",
    "    for track in tracker.tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 5:\n",
    "            continue \n",
    "        bbox = track.to_tlbr() # Get the corrected/predicted bounding box\n",
    "        class_name = track.get_class() #Get the class name of particular object\n",
    "        tracking_id = track.track_id # Get the ID for the particular track\n",
    "        tracked_bboxes.append(bbox.tolist() + [tracking_id]) # Structure data, that we could use it with our draw_bbox function\n",
    "    # 각 track 별로 crop 후 object에 저장\n",
    "    for t in tracked_bboxes:\n",
    "        _t=np.array(t[:4])\n",
    "        _t=np.concatenate([_t[:2][::-1],_t[2:][::-1]])\n",
    "        cropped=tf.image.crop_and_resize(frame[None], np.array([_t[:4]])/(frame.shape[:2]+frame.shape[:2]), [0], (128,64))[0].numpy()\n",
    "        \n",
    "        if not t[4] in objects.keys(): objects[t[4]]=[]\n",
    "        \n",
    "        objects[t[4]].append(cropped)\n",
    "    \n",
    "    # 현재 프레임의 추적 결과를 tracked 리스트에 저장.\n",
    "    tracked.append(tracked_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7a849-eab5-4fec-8852-630f6261f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_sim={}\n",
    "for i in tuple(objects.keys()):\n",
    "    obs=objects[i]\n",
    "    \n",
    "    ret=[]\n",
    "    for o in obs:\n",
    "        feature1=encoder(np.array(target),[[0,0,64,128]])\n",
    "        feature2=encoder(np.array(o),[[0,0,64,128]])\n",
    "        ret.append(np.mean((feature1-feature2)**2)**0.5)\n",
    "    # id 별로 유사도 저장.\n",
    "    objects_sim[i]=ret\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f76619-2f15-45be-a3fe-a8bbd0ecd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates={}\n",
    "for k in objects_sim.keys():\n",
    "    perc=np.percentile(objects_sim[k],10)\n",
    "    #각 id 트랙의 하위10퍼센트 가지고만 평균\n",
    "    sim=np.mean(np.array(objects_sim[k])[objects_sim[k]<=perc])\n",
    "    \n",
    "    #그러면 이제, 각 id 트랙별로 유사도 값은 1개 나옴. 그게 threshold보다 작으면 후보로 선정.\n",
    "    #기존에는 후보들 중에서 가장 거리가 적은게 picked 됐었는데 이걸 앙상블 기법을 이용해야 함.\n",
    "    if sim<=sim_threshold:\n",
    "        candidates[k]=sim\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a22c7a-a7de-4d8c-ac82-34e0c81b449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensemble_similar_id(candidates_dicts, method=\"voting\"):\n",
    "    \"\"\"\n",
    "    앙상블 기법으로 최종 picked_id 결정\n",
    "    :param candidates_dicts: 각 target별 candidates 딕셔너리 리스트\n",
    "    :param method: 'voting' 또는 'mean' 지원\n",
    "    :return: 최종 picked_id\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    all_ids = []\n",
    "    for candidates in candidates_dicts:\n",
    "        picked_id = similar_id(candidates, np.sort(list(candidates.values()))[0])\n",
    "        all_ids.append(picked_id)\n",
    "\n",
    "    if method == \"voting\":\n",
    "        # 다수결로 ID 결정 (원핫 인코딩 + argmax 사용)\n",
    "        unique_ids = list(set(all_ids))  # 고유 ID 추출\n",
    "        id_to_index = {uid: idx for idx, uid in enumerate(unique_ids)}  # ID를 인덱스로 매핑\n",
    "\n",
    "        # 원핫 벡터 초기화\n",
    "        one_hot_counts = np.zeros(len(unique_ids), dtype=int)\n",
    "\n",
    "        # 원핫 벡터에 등장 횟수 누적\n",
    "        for pid in all_ids:\n",
    "            one_hot_counts[id_to_index[pid]] += 1\n",
    "\n",
    "        # argmax로 가장 많이 등장한 ID 선택\n",
    "        final_id = unique_ids[np.argmax(one_hot_counts)]\n",
    "    elif method == \"mean\":\n",
    "        # 평균 유사도가 가장 낮은 ID\n",
    "        mean_scores = {pid: np.mean([cand.get(pid, float(\"inf\")) for cand in candidates_dicts]) for pid in all_ids}\n",
    "        final_id = min(mean_scores, key=mean_scores.get)\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 method: 'voting' 또는 'mean'을 선택하세요.\")\n",
    "\n",
    "    return final_id\n",
    "\n",
    "\n",
    "\n",
    "target_image_paths = [\"/root/jupyter/Final_MP/deepsort_gwanghee/MP/MP/test.png\"]\n",
    "# 수정된 main loop\n",
    "target_images = [iio.imread(image_path)[..., :3] for image_path in target_image_paths]  # 여러 target 이미지 로드\n",
    "candidates_list = []\n",
    "\n",
    "for target in target_images:\n",
    "    # 각 target에 대해 유사도 계산\n",
    "    objects_sim = {}\n",
    "    for i in tuple(objects.keys()):\n",
    "        obs = objects[i]\n",
    "        ret = []\n",
    "        for o in obs:\n",
    "            feature1 = encoder(np.array(target), [[0, 0, 64, 128]])\n",
    "            feature2 = encoder(np.array(o), [[0, 0, 64, 128]])\n",
    "            ret.append(np.mean((feature1 - feature2) ** 2) ** 0.5)\n",
    "        objects_sim[i] = ret\n",
    "\n",
    "    candidates = {}\n",
    "    for k in objects_sim.keys():\n",
    "        perc = np.percentile(objects_sim[k], 10)\n",
    "        sim = np.mean(np.array(objects_sim[k])[objects_sim[k] <= perc])\n",
    "        \n",
    "        #threshold수정(실험)\n",
    "        if sim <= sim_threshold:\n",
    "            candidates[k] = sim\n",
    "\n",
    "    candidates_list.append(candidates)\n",
    "\n",
    "# 앙상블 기법으로 최종 ID 결정\n",
    "picked_id = ensemble_similar_id(candidates_list, method=\"voting\")\n",
    "\n",
    "# 이후 picked_id를 사용한 기존 출력 로직 유지\n",
    "idx = picked_id\n",
    "n = np.argmin(objects_sim[idx])\n",
    "\n",
    "print(\"Target image:\")\n",
    "plt.imshow(target_images[0] / 255)  # 예시: 첫 target\n",
    "plt.show()\n",
    "\n",
    "print(\"Found target:\")\n",
    "plt.imshow(objects[idx][n] / 255)\n",
    "plt.show()\n",
    "\n",
    "# 비디오 출력 (기존 로직 유지)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e05c32-8229-4c03-904f-d8257bfeeb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=picked_id\n",
    "n=np.argmin(objects_sim[idx])\n",
    "\n",
    "print(\"Target image:\")\n",
    "plt.imshow(target/255)\n",
    "plt.show()\n",
    "\n",
    "print(\"Found target:\")\n",
    "plt.imshow(objects[idx][n]/255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817abc11-c90d-4be5-b751-e5c321e528cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 29.7, frames.shape[1:3][::-1])\n",
    "\n",
    "out_frames=[]\n",
    "\n",
    "for i in range(len(frames)):\n",
    "    _frame=frames[i].copy()\n",
    "    for t in tracked[i]:\n",
    "        x1, y1, x2, y2, pid = t\n",
    "\n",
    "        bbox_color=(255,0,0)\n",
    "        if pid==picked_id:\n",
    "            bbox_color=(0,255,0)\n",
    "        elif pid in candidates_list:\n",
    "            bbox_color=(255,255,0)\n",
    "\n",
    "        cv2.rectangle(_frame, (int(x1), int(y1)), (int(x2), int(y2)), bbox_color, 2)\n",
    "        \n",
    "    out.write(_frame[...,::-1])\n",
    "    out_frames.append(_frame)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363dde9c-e75e-44ae-b977-90173caa7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.13.0 numpy==1.24.3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4247c66-ee90-4141-879f-f8fa44ac1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# #def replace_np_float(directory):\n",
    "#     for root, _, files in os.walk(directory):\n",
    "#         for file in files:\n",
    "#             if file.endswith(\".py\"):\n",
    "#                 filepath = os.path.join(root, file)\n",
    "#                 with open(filepath, \"r\") as f:\n",
    "#                     content = f.read()\n",
    "#                 content = content.replace(\"np.float\", \"float\")  # 또는 \"np.float64\"\n",
    "#                 with open(filepath, \"w\") as f:\n",
    "#                     f.write(content)\n",
    "\n",
    "# # 수정할 프로젝트 디렉토리 경로를 입력\n",
    "# replace_np_float(\"/root/jupyter/Final_MP/deepsort_gwanghee/MP/MP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a145c1-c063-47d4-b755-bda96d593832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# 동영상 파일 경로\n",
    "video_path = \"./output.avi\"\n",
    "\n",
    "# 동영상 파일 열기\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# 동영상 재생\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:  # 더 이상 프레임이 없으면 종료\n",
    "        print(\"End of video.\")\n",
    "        break\n",
    "\n",
    "    # 프레임 보여주기\n",
    "    cv2.imshow(\"Video Playback\", frame)\n",
    "\n",
    "    # 'q'를 누르면 종료\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043dc6b0-5f98-4260-8844-7314d15b290e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
